import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import re
import sys

sys.stdout.reconfigure(encoding='utf-8')

# -----------------------------
# 1Ô∏è‚É£ ƒê·ªçc danh s√°ch keyword
# -----------------------------
def load_keywords(filepath):
    with open(filepath, "r", encoding="utf-8") as f:
        keywords = [line.strip().lower() for line in f if line.strip()]
    print(f"üîë ƒê√£ load {len(keywords)} keyword.")
    return keywords

# -----------------------------
# 2Ô∏è‚É£ ƒê·ªçc danh s√°ch URL b√†i b√°o
# -----------------------------
def load_article_links(filepath):
    if filepath.endswith(".json"):
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
        urls = [item["link"] for item in data]
    elif filepath.endswith(".csv"):
        df = pd.read_csv(filepath)
        urls = df["link"].dropna().tolist()
    else:
        raise ValueError("File ph·∫£i l√† .json ho·∫∑c .csv")

    print(f"üì∞ ƒê√£ load {len(urls)} b√†i b√°o.")
    return urls

# -----------------------------
# 3Ô∏è‚É£ H√†m l·∫•y n·ªôi dung b√†i b√°o
# -----------------------------
def get_article_text(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code != 200:
            print(f"‚ö†Ô∏è Kh√¥ng truy c·∫≠p ƒë∆∞·ª£c: {url}")
            return ""
        soup = BeautifulSoup(resp.text, "html.parser")

        # L·∫•y ph·∫ßn n·ªôi dung ch√≠nh
        paragraphs = soup.select("article p")
        text = " ".join(p.get_text(separator=" ", strip=True) for p in paragraphs)
        return text.lower()
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc {url}: {e}")
        return ""

# -----------------------------
# 4Ô∏è‚É£ ƒê·∫øm s·ªë l·∫ßn keyword xu·∫•t hi·ªán
# -----------------------------
def count_keywords_in_text(text, keywords):
    counts = {}
    for kw in keywords:
        # Regex ƒë·ªÉ t√¨m ch√≠nh x√°c t·ª´, kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng
        pattern = r"\b" + re.escape(kw) + r"\b"
        counts[kw] = len(re.findall(pattern, text, flags=re.IGNORECASE))
    return counts

# -----------------------------
# 5Ô∏è‚É£ Ch·∫°y to√†n b·ªô pipeline
# -----------------------------
def main():
    keyword_file = "keywords.txt"            # file ch·ª©a keyword (m·ªói d√≤ng 1 t·ª´)
    article_file = "vnexpress_climate.json"  # file ch·ª©a danh s√°ch b√†i b√°o
    output_file = "keyword_count.csv"

    keywords = load_keywords(keyword_file)
    urls = load_article_links(article_file)

    results = []

    for idx, url in enumerate(urls, start=1):
        print(f"üìÑ [{idx}/{len(urls)}] ƒêang x·ª≠ l√Ω: {url}")
        text = get_article_text(url)
        if not text:
            continue
        kw_counts = count_keywords_in_text(text, keywords)
        row = {"url": url, **kw_counts}
        results.append(row)

    # Xu·∫•t k·∫øt qu·∫£ ra CSV
    df = pd.DataFrame(results)
    df.to_csv(output_file, index=False, encoding="utf-8-sig")

    print(f"\n‚úÖ Ho√†n t·∫•t! K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u v√†o '{output_file}' ({len(results)} b√†i).")

if __name__ == "__main__":
    main()
